{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "visualising_and_Interpreting_cnns_workbook.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0u3fvtulCNHU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumendra/cnn-visualisation/blob/master/notebooks/visualising_and_interpreting_cnns_workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Jd1S64aMCNHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "![](https://github.com/soumendra/cnn-visualisation/blob/master/notebooks/assets/promiscuous_dlmodel.png?raw=1)\n",
        "\n",
        "1. Build a small model\n",
        "2. Visualisation technique 1\n",
        "3. Visualisation technique 2\n",
        "4. Visualisation technique 3\n",
        "5. Real-life Demo\n",
        "\n",
        "# Colab specific updates"
      ]
    },
    {
      "metadata": {
        "id": "fT-vnunJEdgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "10b3e5e9-48e7-4fc0-f401-069bdd4460d2"
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/raghakot/keras-vis.git --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/raghakot/keras-vis.git\n",
            "  Cloning https://github.com/raghakot/keras-vis.git to /tmp/pip-req-build-4bqxnsn2\n",
            "Requirement already satisfied, skipping upgrade: keras in /usr/local/lib/python3.6/dist-packages (from keras-vis==0.4.1) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from keras-vis==0.4.1) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from keras-vis==0.4.1) (0.13.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from keras-vis==0.4.1) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-vis==0.4.1) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vis==0.4.1) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-vis==0.4.1) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vis==0.4.1) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vis==0.4.1) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vis==0.4.1) (1.0.6)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->keras-vis==0.4.1) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image->keras-vis==0.4.1) (2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->keras-vis==0.4.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-vis==0.4.1) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-vis==0.4.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-vis==0.4.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->keras-vis==0.4.1) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=2.1.0->scikit-image->keras-vis==0.4.1) (0.46)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image->keras-vis==0.4.1) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->keras-vis==0.4.1) (40.6.3)\n",
            "Building wheels for collected packages: keras-vis\n",
            "  Running setup.py bdist_wheel for keras-vis ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-n2o8zrtb/wheels/c5/ae/e7/b34d1cb48b1898f606a5cce08ebc9521fa0588f37f1e590d9f\n",
            "Successfully built keras-vis\n",
            "Installing collected packages: keras-vis\n",
            "  Found existing installation: keras-vis 0.4.1\n",
            "    Uninstalling keras-vis-0.4.1:\n",
            "      Successfully uninstalled keras-vis-0.4.1\n",
            "Successfully installed keras-vis-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tg0vSaSLCNHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build a small MNIST Model\n",
        "\n",
        "* Step 1: Build model\n",
        "* Step 2: Compile\n",
        "* Step 3: Train\n",
        "* Step 4: Test\n",
        "\n",
        "## Data Prep"
      ]
    },
    {
      "metadata": {
        "id": "bXG63oKLCNHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8f0bca56-9bd6-4df9-b017-8d9319533a2d"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mcsLwSaqCNHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The MNIST Model"
      ]
    },
    {
      "metadata": {
        "id": "tNQxyB1JCNHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "43e61157-73e2-40e3-b5d7-2f5dd1500f44"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax', name='preds'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.2360 - acc: 0.9276 - val_loss: 0.0486 - val_acc: 0.9836\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.0824 - acc: 0.9749 - val_loss: 0.0407 - val_acc: 0.9856\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0632 - acc: 0.9807 - val_loss: 0.0348 - val_acc: 0.9889\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0523 - acc: 0.9840 - val_loss: 0.0342 - val_acc: 0.9888\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0465 - acc: 0.9851 - val_loss: 0.0292 - val_acc: 0.9896\n",
            "Test loss: 0.029239640416105977\n",
            "Test accuracy: 0.9896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8rJtWhp9CNHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "ad37eee4-0d54-4401-ff76-a3e3ddfb8835"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "preds (Dense)                (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YcRK6y9nDjvY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dense Layer Visualizations\n",
        "\n",
        "* To visualize activation over final dense layer outputs, we need to switch the softmax activation out for linear since gradient of output node will depend on all the other node activations.\n",
        "\n",
        "* Doing this in keras is tricky, so we provide utils.apply_modifications to modify network parameters and rebuild the graph.\n",
        "\n",
        "* If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear' and compare what happens if we dont do this at the end.\n",
        "\n",
        "* Lets start by visualizing input that maximizes the output of node 0. Hopefully this looks like a 0."
      ]
    },
    {
      "metadata": {
        "id": "R3inhCORDFSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b2a929da-4a9f-4619-dec9-754bcf3e73e3"
      },
      "cell_type": "code",
      "source": [
        "from vis.visualization import visualize_activation\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (18, 6)\n",
        "\n",
        "# Utility to search for layer index by name. \n",
        "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
        "layer_idx = utils.find_layer_idx(model, 'preds')\n",
        "\n",
        "# Swap softmax with linear\n",
        "model.layers[layer_idx].activation = activations.linear\n",
        "model = utils.apply_modifications(model)\n",
        "\n",
        "# This is the output node we want to maximize.\n",
        "filter_idx = 0\n",
        "img = visualize_activation(model, layer_idx, filter_indices=filter_idx)\n",
        "plt.imshow(img[..., 0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe0267e2390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAFlCAYAAADYskK4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSMwAkBkSMwBkhsQMAJkhMQNAZv4/oEQ5o1ONGnMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jcx7yL7SFXtl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hmm, it sort of looks like a 0, but not as clear as we hoped for. Activation maximization is notorious because regularization parameters needs to be tuned depending on the problem. Lets enumerate all the possible reasons why this didn't work very well.\n",
        "\n",
        "The input to network is preprocessed to range (0, 1). We should specify input_range = (0., 1.) to constrain the input to this range.\n",
        "The regularization parameter default weights might be dominating activation maximization loss weight. One way to debug this is to use verbose=True and examine individual loss values.\n",
        "Lets do these step by step and see if we can improve it.\n",
        "\n",
        "# Debugging step 1: Specifying input_range\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g4Lw734bDFEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "00caaac3-918b-478d-cf2b-597fe2cb21ef"
      },
      "cell_type": "code",
      "source": [
        "img = visualize_activation(model, layer_idx, filter_indices=filter_idx, input_range=(0., 1.))\n",
        "plt.imshow(img[..., 0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe02453e198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAFlCAYAAADYskK4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxAwAkSExA0Bk/j+LyoPpBfkVKQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ZejJLhfXFj-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Much better but still seems noisy. Lets examining the losses with verbose=True and tuning the weights.\n",
        "\n",
        "# Debugging step 2: Tuning regularization weights\n",
        "\n",
        "One of the issues with activation maximization is that the input can go out of the training distribution space. Total variation and L-p norm are used to provide some hardcoded image priors for natural images. For example, Total variation ensures that images are blobber and not scattered. Unfotunately, sometimes these losses can dominate the main ActivationMaximization loss.\n",
        "\n",
        "Lets see what individual losses are, with verbose=True"
      ]
    },
    {
      "metadata": {
        "id": "AVMAMf7HCNHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10591
        },
        "outputId": "1f983bd9-ff95-465c-f508-247d69652c36"
      },
      "cell_type": "code",
      "source": [
        "img = visualize_activation(model, layer_idx, filter_indices=filter_idx, input_range=(0., 1.), verbose=True)\n",
        "plt.imshow(img[..., 0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1, named_losses: [('ActivationMax Loss', 0.0281808),\n",
            " ('L-6.0 Norm Loss', 0.019829264),\n",
            " ('TV(2.0) Loss', 0.08899963)], overall loss: 0.13700969517230988\n",
            "Iteration: 2, named_losses: [('ActivationMax Loss', 5.923728),\n",
            " ('L-6.0 Norm Loss', 0.1730701),\n",
            " ('TV(2.0) Loss', 611.42487)], overall loss: 617.5216674804688\n",
            "Iteration: 3, named_losses: [('ActivationMax Loss', -50.545033),\n",
            " ('L-6.0 Norm Loss', 0.1799535),\n",
            " ('TV(2.0) Loss', 137.45474)], overall loss: 87.08966064453125\n",
            "Iteration: 4, named_losses: [('ActivationMax Loss', -112.270004),\n",
            " ('L-6.0 Norm Loss', 0.19367224),\n",
            " ('TV(2.0) Loss', 130.8023)], overall loss: 18.725975036621094\n",
            "Iteration: 5, named_losses: [('ActivationMax Loss', -198.67648),\n",
            " ('L-6.0 Norm Loss', 0.21507221),\n",
            " ('TV(2.0) Loss', 117.25624)], overall loss: -81.20516967773438\n",
            "Iteration: 6, named_losses: [('ActivationMax Loss', -308.8051),\n",
            " ('L-6.0 Norm Loss', 0.2504576),\n",
            " ('TV(2.0) Loss', 135.89627)], overall loss: -172.65838623046875\n",
            "Iteration: 7, named_losses: [('ActivationMax Loss', -382.42926),\n",
            " ('L-6.0 Norm Loss', 0.28530008),\n",
            " ('TV(2.0) Loss', 166.90062)], overall loss: -215.2433319091797\n",
            "Iteration: 8, named_losses: [('ActivationMax Loss', -464.85324),\n",
            " ('L-6.0 Norm Loss', 0.30665436),\n",
            " ('TV(2.0) Loss', 189.95532)], overall loss: -274.5912780761719\n",
            "Iteration: 9, named_losses: [('ActivationMax Loss', -534.1804),\n",
            " ('L-6.0 Norm Loss', 0.3392105),\n",
            " ('TV(2.0) Loss', 227.51817)], overall loss: -306.322998046875\n",
            "Iteration: 10, named_losses: [('ActivationMax Loss', -598.7262),\n",
            " ('L-6.0 Norm Loss', 0.36093572),\n",
            " ('TV(2.0) Loss', 250.20868)], overall loss: -348.15655517578125\n",
            "Iteration: 11, named_losses: [('ActivationMax Loss', -657.576),\n",
            " ('L-6.0 Norm Loss', 0.38638803),\n",
            " ('TV(2.0) Loss', 288.9224)], overall loss: -368.2671813964844\n",
            "Iteration: 12, named_losses: [('ActivationMax Loss', -706.3678),\n",
            " ('L-6.0 Norm Loss', 0.4023692),\n",
            " ('TV(2.0) Loss', 306.8595)], overall loss: -399.10595703125\n",
            "Iteration: 13, named_losses: [('ActivationMax Loss', -747.7056),\n",
            " ('L-6.0 Norm Loss', 0.4199192),\n",
            " ('TV(2.0) Loss', 331.6893)], overall loss: -415.5964050292969\n",
            "Iteration: 14, named_losses: [('ActivationMax Loss', -786.8645),\n",
            " ('L-6.0 Norm Loss', 0.4418725),\n",
            " ('TV(2.0) Loss', 351.29044)], overall loss: -435.1321716308594\n",
            "Iteration: 15, named_losses: [('ActivationMax Loss', -817.7693),\n",
            " ('L-6.0 Norm Loss', 0.45482737),\n",
            " ('TV(2.0) Loss', 375.27142)], overall loss: -442.04302978515625\n",
            "Iteration: 16, named_losses: [('ActivationMax Loss', -850.142),\n",
            " ('L-6.0 Norm Loss', 0.47466394),\n",
            " ('TV(2.0) Loss', 383.46255)], overall loss: -466.2048034667969\n",
            "Iteration: 17, named_losses: [('ActivationMax Loss', -867.9265),\n",
            " ('L-6.0 Norm Loss', 0.48497272),\n",
            " ('TV(2.0) Loss', 398.72327)], overall loss: -468.71826171875\n",
            "Iteration: 18, named_losses: [('ActivationMax Loss', -900.15656),\n",
            " ('L-6.0 Norm Loss', 0.50190973),\n",
            " ('TV(2.0) Loss', 417.3663)], overall loss: -482.2883605957031\n",
            "Iteration: 19, named_losses: [('ActivationMax Loss', -914.6732),\n",
            " ('L-6.0 Norm Loss', 0.5084883),\n",
            " ('TV(2.0) Loss', 424.2224)], overall loss: -489.94232177734375\n",
            "Iteration: 20, named_losses: [('ActivationMax Loss', -939.1311),\n",
            " ('L-6.0 Norm Loss', 0.5197185),\n",
            " ('TV(2.0) Loss', 437.7252)], overall loss: -500.8861999511719\n",
            "Iteration: 21, named_losses: [('ActivationMax Loss', -957.32404),\n",
            " ('L-6.0 Norm Loss', 0.53136736),\n",
            " ('TV(2.0) Loss', 451.12054)], overall loss: -505.672119140625\n",
            "Iteration: 22, named_losses: [('ActivationMax Loss', -977.0768),\n",
            " ('L-6.0 Norm Loss', 0.5390955),\n",
            " ('TV(2.0) Loss', 461.41818)], overall loss: -515.1195068359375\n",
            "Iteration: 23, named_losses: [('ActivationMax Loss', -990.4885),\n",
            " ('L-6.0 Norm Loss', 0.54935956),\n",
            " ('TV(2.0) Loss', 467.63177)], overall loss: -522.307373046875\n",
            "Iteration: 24, named_losses: [('ActivationMax Loss', -1008.9744),\n",
            " ('L-6.0 Norm Loss', 0.56305856),\n",
            " ('TV(2.0) Loss', 484.312)], overall loss: -524.099365234375\n",
            "Iteration: 25, named_losses: [('ActivationMax Loss', -1017.6732),\n",
            " ('L-6.0 Norm Loss', 0.56956947),\n",
            " ('TV(2.0) Loss', 486.68927)], overall loss: -530.4143676757812\n",
            "Iteration: 26, named_losses: [('ActivationMax Loss', -1035.939),\n",
            " ('L-6.0 Norm Loss', 0.5757663),\n",
            " ('TV(2.0) Loss', 503.91006)], overall loss: -531.453125\n",
            "Iteration: 27, named_losses: [('ActivationMax Loss', -1039.1364),\n",
            " ('L-6.0 Norm Loss', 0.5832774),\n",
            " ('TV(2.0) Loss', 502.39212)], overall loss: -536.1610107421875\n",
            "Iteration: 28, named_losses: [('ActivationMax Loss', -1051.2423),\n",
            " ('L-6.0 Norm Loss', 0.5875374),\n",
            " ('TV(2.0) Loss', 512.1419)], overall loss: -538.5128784179688\n",
            "Iteration: 29, named_losses: [('ActivationMax Loss', -1054.7352),\n",
            " ('L-6.0 Norm Loss', 0.59411937),\n",
            " ('TV(2.0) Loss', 510.98724)], overall loss: -543.1538696289062\n",
            "Iteration: 30, named_losses: [('ActivationMax Loss', -1070.9552),\n",
            " ('L-6.0 Norm Loss', 0.6032476),\n",
            " ('TV(2.0) Loss', 526.59644)], overall loss: -543.7554931640625\n",
            "Iteration: 31, named_losses: [('ActivationMax Loss', -1071.593),\n",
            " ('L-6.0 Norm Loss', 0.60665196),\n",
            " ('TV(2.0) Loss', 525.92236)], overall loss: -545.06396484375\n",
            "Iteration: 32, named_losses: [('ActivationMax Loss', -1086.644),\n",
            " ('L-6.0 Norm Loss', 0.6121821),\n",
            " ('TV(2.0) Loss', 538.96747)], overall loss: -547.0643920898438\n",
            "Iteration: 33, named_losses: [('ActivationMax Loss', -1087.2596),\n",
            " ('L-6.0 Norm Loss', 0.61661434),\n",
            " ('TV(2.0) Loss', 537.8603)], overall loss: -548.7827758789062\n",
            "Iteration: 34, named_losses: [('ActivationMax Loss', -1095.245),\n",
            " ('L-6.0 Norm Loss', 0.6219945),\n",
            " ('TV(2.0) Loss', 543.7736)], overall loss: -550.8494262695312\n",
            "Iteration: 35, named_losses: [('ActivationMax Loss', -1097.0599),\n",
            " ('L-6.0 Norm Loss', 0.62552905),\n",
            " ('TV(2.0) Loss', 545.4916)], overall loss: -550.94287109375\n",
            "Iteration: 36, named_losses: [('ActivationMax Loss', -1104.0455),\n",
            " ('L-6.0 Norm Loss', 0.6290181),\n",
            " ('TV(2.0) Loss', 549.0493)], overall loss: -554.3671875\n",
            "Iteration: 37, named_losses: [('ActivationMax Loss', -1107.702),\n",
            " ('L-6.0 Norm Loss', 0.6324055),\n",
            " ('TV(2.0) Loss', 552.54517)], overall loss: -554.5244140625\n",
            "Iteration: 38, named_losses: [('ActivationMax Loss', -1115.7736),\n",
            " ('L-6.0 Norm Loss', 0.63516366),\n",
            " ('TV(2.0) Loss', 558.3549)], overall loss: -556.7835083007812\n",
            "Iteration: 39, named_losses: [('ActivationMax Loss', -1109.6865),\n",
            " ('L-6.0 Norm Loss', 0.6388864),\n",
            " ('TV(2.0) Loss', 554.0142)], overall loss: -555.0333862304688\n",
            "Iteration: 40, named_losses: [('ActivationMax Loss', -1124.4208),\n",
            " ('L-6.0 Norm Loss', 0.6437291),\n",
            " ('TV(2.0) Loss', 566.6796)], overall loss: -557.0974731445312\n",
            "Iteration: 41, named_losses: [('ActivationMax Loss', -1119.289),\n",
            " ('L-6.0 Norm Loss', 0.64486307),\n",
            " ('TV(2.0) Loss', 560.95514)], overall loss: -557.6889038085938\n",
            "Iteration: 42, named_losses: [('ActivationMax Loss', -1128.3558),\n",
            " ('L-6.0 Norm Loss', 0.64851755),\n",
            " ('TV(2.0) Loss', 567.5272)], overall loss: -560.1800537109375\n",
            "Iteration: 43, named_losses: [('ActivationMax Loss', -1124.8972),\n",
            " ('L-6.0 Norm Loss', 0.6475952),\n",
            " ('TV(2.0) Loss', 565.0656)], overall loss: -559.1840209960938\n",
            "Iteration: 44, named_losses: [('ActivationMax Loss', -1131.2616),\n",
            " ('L-6.0 Norm Loss', 0.6544095),\n",
            " ('TV(2.0) Loss', 568.61194)], overall loss: -561.9952392578125\n",
            "Iteration: 45, named_losses: [('ActivationMax Loss', -1133.4445),\n",
            " ('L-6.0 Norm Loss', 0.65430933),\n",
            " ('TV(2.0) Loss', 573.95026)], overall loss: -558.8399047851562\n",
            "Iteration: 46, named_losses: [('ActivationMax Loss', -1142.6256),\n",
            " ('L-6.0 Norm Loss', 0.65974176),\n",
            " ('TV(2.0) Loss', 579.98126)], overall loss: -561.9845581054688\n",
            "Iteration: 47, named_losses: [('ActivationMax Loss', -1136.0411),\n",
            " ('L-6.0 Norm Loss', 0.65849966),\n",
            " ('TV(2.0) Loss', 575.66406)], overall loss: -559.7186279296875\n",
            "Iteration: 48, named_losses: [('ActivationMax Loss', -1145.1681),\n",
            " ('L-6.0 Norm Loss', 0.6619837),\n",
            " ('TV(2.0) Loss', 584.1313)], overall loss: -560.3748168945312\n",
            "Iteration: 49, named_losses: [('ActivationMax Loss', -1138.6129),\n",
            " ('L-6.0 Norm Loss', 0.66330564),\n",
            " ('TV(2.0) Loss', 576.98584)], overall loss: -560.9637451171875\n",
            "Iteration: 50, named_losses: [('ActivationMax Loss', -1148.461),\n",
            " ('L-6.0 Norm Loss', 0.6647433),\n",
            " ('TV(2.0) Loss', 588.0704)], overall loss: -559.7258911132812\n",
            "Iteration: 51, named_losses: [('ActivationMax Loss', -1142.2416),\n",
            " ('L-6.0 Norm Loss', 0.6658744),\n",
            " ('TV(2.0) Loss', 577.65625)], overall loss: -563.91943359375\n",
            "Iteration: 52, named_losses: [('ActivationMax Loss', -1148.503),\n",
            " ('L-6.0 Norm Loss', 0.66699624),\n",
            " ('TV(2.0) Loss', 586.3891)], overall loss: -561.4469604492188\n",
            "Iteration: 53, named_losses: [('ActivationMax Loss', -1142.9391),\n",
            " ('L-6.0 Norm Loss', 0.668196),\n",
            " ('TV(2.0) Loss', 578.8779)], overall loss: -563.3929443359375\n",
            "Iteration: 54, named_losses: [('ActivationMax Loss', -1152.9146),\n",
            " ('L-6.0 Norm Loss', 0.67075473),\n",
            " ('TV(2.0) Loss', 592.8578)], overall loss: -559.385986328125\n",
            "Iteration: 55, named_losses: [('ActivationMax Loss', -1148.8687),\n",
            " ('L-6.0 Norm Loss', 0.66949344),\n",
            " ('TV(2.0) Loss', 584.166)], overall loss: -564.033203125\n",
            "Iteration: 56, named_losses: [('ActivationMax Loss', -1148.2716),\n",
            " ('L-6.0 Norm Loss', 0.6724684),\n",
            " ('TV(2.0) Loss', 586.35596)], overall loss: -561.2431640625\n",
            "Iteration: 57, named_losses: [('ActivationMax Loss', -1148.2297),\n",
            " ('L-6.0 Norm Loss', 0.67097926),\n",
            " ('TV(2.0) Loss', 583.0071)], overall loss: -564.5516357421875\n",
            "Iteration: 58, named_losses: [('ActivationMax Loss', -1153.2284),\n",
            " ('L-6.0 Norm Loss', 0.6734034),\n",
            " ('TV(2.0) Loss', 588.15564)], overall loss: -564.3992919921875\n",
            "Iteration: 59, named_losses: [('ActivationMax Loss', -1149.014),\n",
            " ('L-6.0 Norm Loss', 0.6753294),\n",
            " ('TV(2.0) Loss', 583.8238)], overall loss: -564.5149536132812\n",
            "Iteration: 60, named_losses: [('ActivationMax Loss', -1159.3163),\n",
            " ('L-6.0 Norm Loss', 0.6796384),\n",
            " ('TV(2.0) Loss', 595.40936)], overall loss: -563.2272338867188\n",
            "Iteration: 61, named_losses: [('ActivationMax Loss', -1151.3812),\n",
            " ('L-6.0 Norm Loss', 0.6765212),\n",
            " ('TV(2.0) Loss', 586.41504)], overall loss: -564.2896728515625\n",
            "Iteration: 62, named_losses: [('ActivationMax Loss', -1160.366),\n",
            " ('L-6.0 Norm Loss', 0.68067366),\n",
            " ('TV(2.0) Loss', 595.9895)], overall loss: -563.69580078125\n",
            "Iteration: 63, named_losses: [('ActivationMax Loss', -1152.5399),\n",
            " ('L-6.0 Norm Loss', 0.6784717),\n",
            " ('TV(2.0) Loss', 587.1748)], overall loss: -564.6866455078125\n",
            "Iteration: 64, named_losses: [('ActivationMax Loss', -1167.2421),\n",
            " ('L-6.0 Norm Loss', 0.68301916),\n",
            " ('TV(2.0) Loss', 602.9075)], overall loss: -563.651611328125\n",
            "Iteration: 65, named_losses: [('ActivationMax Loss', -1156.3336),\n",
            " ('L-6.0 Norm Loss', 0.6794746),\n",
            " ('TV(2.0) Loss', 591.61755)], overall loss: -564.03662109375\n",
            "Iteration: 66, named_losses: [('ActivationMax Loss', -1167.0792),\n",
            " ('L-6.0 Norm Loss', 0.68439966),\n",
            " ('TV(2.0) Loss', 602.6464)], overall loss: -563.7483520507812\n",
            "Iteration: 67, named_losses: [('ActivationMax Loss', -1158.882),\n",
            " ('L-6.0 Norm Loss', 0.6821089),\n",
            " ('TV(2.0) Loss', 594.3789)], overall loss: -563.8209228515625\n",
            "Iteration: 68, named_losses: [('ActivationMax Loss', -1166.1718),\n",
            " ('L-6.0 Norm Loss', 0.6862236),\n",
            " ('TV(2.0) Loss', 599.70355)], overall loss: -565.7819213867188\n",
            "Iteration: 69, named_losses: [('ActivationMax Loss', -1159.6735),\n",
            " ('L-6.0 Norm Loss', 0.6843688),\n",
            " ('TV(2.0) Loss', 594.0233)], overall loss: -564.9658203125\n",
            "Iteration: 70, named_losses: [('ActivationMax Loss', -1167.3472),\n",
            " ('L-6.0 Norm Loss', 0.6863601),\n",
            " ('TV(2.0) Loss', 598.76935)], overall loss: -567.8914184570312\n",
            "Iteration: 71, named_losses: [('ActivationMax Loss', -1162.2694),\n",
            " ('L-6.0 Norm Loss', 0.68601555),\n",
            " ('TV(2.0) Loss', 597.387)], overall loss: -564.1963500976562\n",
            "Iteration: 72, named_losses: [('ActivationMax Loss', -1170.9673),\n",
            " ('L-6.0 Norm Loss', 0.68800277),\n",
            " ('TV(2.0) Loss', 603.0858)], overall loss: -567.1934814453125\n",
            "Iteration: 73, named_losses: [('ActivationMax Loss', -1162.853),\n",
            " ('L-6.0 Norm Loss', 0.68596405),\n",
            " ('TV(2.0) Loss', 598.24927)], overall loss: -563.9178466796875\n",
            "Iteration: 74, named_losses: [('ActivationMax Loss', -1170.7356),\n",
            " ('L-6.0 Norm Loss', 0.68860775),\n",
            " ('TV(2.0) Loss', 602.28107)], overall loss: -567.7659301757812\n",
            "Iteration: 75, named_losses: [('ActivationMax Loss', -1162.8492),\n",
            " ('L-6.0 Norm Loss', 0.68719876),\n",
            " ('TV(2.0) Loss', 599.27496)], overall loss: -562.8870239257812\n",
            "Iteration: 76, named_losses: [('ActivationMax Loss', -1173.5234),\n",
            " ('L-6.0 Norm Loss', 0.68959606),\n",
            " ('TV(2.0) Loss', 604.6641)], overall loss: -568.1697387695312\n",
            "Iteration: 77, named_losses: [('ActivationMax Loss', -1163.1974),\n",
            " ('L-6.0 Norm Loss', 0.68751746),\n",
            " ('TV(2.0) Loss', 598.18616)], overall loss: -564.32373046875\n",
            "Iteration: 78, named_losses: [('ActivationMax Loss', -1174.7728),\n",
            " ('L-6.0 Norm Loss', 0.6896427),\n",
            " ('TV(2.0) Loss', 606.8419)], overall loss: -567.2412109375\n",
            "Iteration: 79, named_losses: [('ActivationMax Loss', -1162.76),\n",
            " ('L-6.0 Norm Loss', 0.6902659),\n",
            " ('TV(2.0) Loss', 599.26587)], overall loss: -562.8038330078125\n",
            "Iteration: 80, named_losses: [('ActivationMax Loss', -1175.7921),\n",
            " ('L-6.0 Norm Loss', 0.6918899),\n",
            " ('TV(2.0) Loss', 608.3336)], overall loss: -566.7666015625\n",
            "Iteration: 81, named_losses: [('ActivationMax Loss', -1165.1595),\n",
            " ('L-6.0 Norm Loss', 0.6910923),\n",
            " ('TV(2.0) Loss', 600.10614)], overall loss: -564.3623657226562\n",
            "Iteration: 82, named_losses: [('ActivationMax Loss', -1173.775),\n",
            " ('L-6.0 Norm Loss', 0.6926553),\n",
            " ('TV(2.0) Loss', 606.6599)], overall loss: -566.4224853515625\n",
            "Iteration: 83, named_losses: [('ActivationMax Loss', -1168.4376),\n",
            " ('L-6.0 Norm Loss', 0.6919529),\n",
            " ('TV(2.0) Loss', 603.7341)], overall loss: -564.0116577148438\n",
            "Iteration: 84, named_losses: [('ActivationMax Loss', -1170.1776),\n",
            " ('L-6.0 Norm Loss', 0.69082683),\n",
            " ('TV(2.0) Loss', 603.4957)], overall loss: -565.9910888671875\n",
            "Iteration: 85, named_losses: [('ActivationMax Loss', -1167.0679),\n",
            " ('L-6.0 Norm Loss', 0.69229394),\n",
            " ('TV(2.0) Loss', 600.33453)], overall loss: -566.0410766601562\n",
            "Iteration: 86, named_losses: [('ActivationMax Loss', -1171.5762),\n",
            " ('L-6.0 Norm Loss', 0.69179595),\n",
            " ('TV(2.0) Loss', 604.6313)], overall loss: -566.2531127929688\n",
            "Iteration: 87, named_losses: [('ActivationMax Loss', -1169.3463),\n",
            " ('L-6.0 Norm Loss', 0.6921128),\n",
            " ('TV(2.0) Loss', 602.2673)], overall loss: -566.3869018554688\n",
            "Iteration: 88, named_losses: [('ActivationMax Loss', -1175.4325),\n",
            " ('L-6.0 Norm Loss', 0.69329625),\n",
            " ('TV(2.0) Loss', 607.50433)], overall loss: -567.2349243164062\n",
            "Iteration: 89, named_losses: [('ActivationMax Loss', -1164.1283),\n",
            " ('L-6.0 Norm Loss', 0.6919095),\n",
            " ('TV(2.0) Loss', 598.6665)], overall loss: -564.7698974609375\n",
            "Iteration: 90, named_losses: [('ActivationMax Loss', -1172.9557),\n",
            " ('L-6.0 Norm Loss', 0.6931835),\n",
            " ('TV(2.0) Loss', 605.39307)], overall loss: -566.869384765625\n",
            "Iteration: 91, named_losses: [('ActivationMax Loss', -1163.9231),\n",
            " ('L-6.0 Norm Loss', 0.6914244),\n",
            " ('TV(2.0) Loss', 595.5759)], overall loss: -567.65576171875\n",
            "Iteration: 92, named_losses: [('ActivationMax Loss', -1173.6989),\n",
            " ('L-6.0 Norm Loss', 0.6950034),\n",
            " ('TV(2.0) Loss', 605.1621)], overall loss: -567.841796875\n",
            "Iteration: 93, named_losses: [('ActivationMax Loss', -1166.6372),\n",
            " ('L-6.0 Norm Loss', 0.69320583),\n",
            " ('TV(2.0) Loss', 599.31946)], overall loss: -566.62451171875\n",
            "Iteration: 94, named_losses: [('ActivationMax Loss', -1173.7135),\n",
            " ('L-6.0 Norm Loss', 0.69492733),\n",
            " ('TV(2.0) Loss', 604.97125)], overall loss: -568.0473022460938\n",
            "Iteration: 95, named_losses: [('ActivationMax Loss', -1169.3115),\n",
            " ('L-6.0 Norm Loss', 0.6952822),\n",
            " ('TV(2.0) Loss', 599.20734)], overall loss: -569.4088745117188\n",
            "Iteration: 96, named_losses: [('ActivationMax Loss', -1170.9742),\n",
            " ('L-6.0 Norm Loss', 0.6949936),\n",
            " ('TV(2.0) Loss', 602.99536)], overall loss: -567.283935546875\n",
            "Iteration: 97, named_losses: [('ActivationMax Loss', -1169.6967),\n",
            " ('L-6.0 Norm Loss', 0.6968583),\n",
            " ('TV(2.0) Loss', 602.5023)], overall loss: -566.4974365234375\n",
            "Iteration: 98, named_losses: [('ActivationMax Loss', -1177.7557),\n",
            " ('L-6.0 Norm Loss', 0.69646275),\n",
            " ('TV(2.0) Loss', 610.5713)], overall loss: -566.488037109375\n",
            "Iteration: 99, named_losses: [('ActivationMax Loss', -1169.2821),\n",
            " ('L-6.0 Norm Loss', 0.69944626),\n",
            " ('TV(2.0) Loss', 602.586)], overall loss: -565.9966430664062\n",
            "Iteration: 100, named_losses: [('ActivationMax Loss', -1179.0914),\n",
            " ('L-6.0 Norm Loss', 0.69702095),\n",
            " ('TV(2.0) Loss', 612.19855)], overall loss: -566.1958618164062\n",
            "Iteration: 101, named_losses: [('ActivationMax Loss', -1168.9194),\n",
            " ('L-6.0 Norm Loss', 0.6941751),\n",
            " ('TV(2.0) Loss', 595.8758)], overall loss: -572.3494262695312\n",
            "Iteration: 102, named_losses: [('ActivationMax Loss', -1176.4972),\n",
            " ('L-6.0 Norm Loss', 0.6984165),\n",
            " ('TV(2.0) Loss', 609.94476)], overall loss: -565.8540649414062\n",
            "Iteration: 103, named_losses: [('ActivationMax Loss', -1170.6345),\n",
            " ('L-6.0 Norm Loss', 0.69574124),\n",
            " ('TV(2.0) Loss', 599.10925)], overall loss: -570.8294677734375\n",
            "Iteration: 104, named_losses: [('ActivationMax Loss', -1176.5872),\n",
            " ('L-6.0 Norm Loss', 0.69783187),\n",
            " ('TV(2.0) Loss', 611.25806)], overall loss: -564.6312255859375\n",
            "Iteration: 105, named_losses: [('ActivationMax Loss', -1173.247),\n",
            " ('L-6.0 Norm Loss', 0.6968215),\n",
            " ('TV(2.0) Loss', 604.4216)], overall loss: -568.1286010742188\n",
            "Iteration: 106, named_losses: [('ActivationMax Loss', -1175.9086),\n",
            " ('L-6.0 Norm Loss', 0.6981839),\n",
            " ('TV(2.0) Loss', 608.31146)], overall loss: -566.8988647460938\n",
            "Iteration: 107, named_losses: [('ActivationMax Loss', -1172.7329),\n",
            " ('L-6.0 Norm Loss', 0.6977225),\n",
            " ('TV(2.0) Loss', 602.46875)], overall loss: -569.56640625\n",
            "Iteration: 108, named_losses: [('ActivationMax Loss', -1177.5865),\n",
            " ('L-6.0 Norm Loss', 0.6989392),\n",
            " ('TV(2.0) Loss', 610.97003)], overall loss: -565.9175415039062\n",
            "Iteration: 109, named_losses: [('ActivationMax Loss', -1171.9656),\n",
            " ('L-6.0 Norm Loss', 0.697486),\n",
            " ('TV(2.0) Loss', 601.4609)], overall loss: -569.8071899414062\n",
            "Iteration: 110, named_losses: [('ActivationMax Loss', -1180.5038),\n",
            " ('L-6.0 Norm Loss', 0.70049155),\n",
            " ('TV(2.0) Loss', 614.65314)], overall loss: -565.1502075195312\n",
            "Iteration: 111, named_losses: [('ActivationMax Loss', -1175.2115),\n",
            " ('L-6.0 Norm Loss', 0.6987121),\n",
            " ('TV(2.0) Loss', 604.8989)], overall loss: -569.6138916015625\n",
            "Iteration: 112, named_losses: [('ActivationMax Loss', -1174.4557),\n",
            " ('L-6.0 Norm Loss', 0.69913185),\n",
            " ('TV(2.0) Loss', 607.7067)], overall loss: -566.0498657226562\n",
            "Iteration: 113, named_losses: [('ActivationMax Loss', -1171.8262),\n",
            " ('L-6.0 Norm Loss', 0.699634),\n",
            " ('TV(2.0) Loss', 602.5322)], overall loss: -568.5943603515625\n",
            "Iteration: 114, named_losses: [('ActivationMax Loss', -1177.827),\n",
            " ('L-6.0 Norm Loss', 0.70065933),\n",
            " ('TV(2.0) Loss', 609.2657)], overall loss: -567.8606567382812\n",
            "Iteration: 115, named_losses: [('ActivationMax Loss', -1172.6337),\n",
            " ('L-6.0 Norm Loss', 0.7022267),\n",
            " ('TV(2.0) Loss', 603.11743)], overall loss: -568.81396484375\n",
            "Iteration: 116, named_losses: [('ActivationMax Loss', -1176.484),\n",
            " ('L-6.0 Norm Loss', 0.70104754),\n",
            " ('TV(2.0) Loss', 607.32495)], overall loss: -568.4580078125\n",
            "Iteration: 117, named_losses: [('ActivationMax Loss', -1172.2386),\n",
            " ('L-6.0 Norm Loss', 0.70330864),\n",
            " ('TV(2.0) Loss', 604.0354)], overall loss: -567.4998779296875\n",
            "Iteration: 118, named_losses: [('ActivationMax Loss', -1182.3485),\n",
            " ('L-6.0 Norm Loss', 0.70178175),\n",
            " ('TV(2.0) Loss', 612.124)], overall loss: -569.522705078125\n",
            "Iteration: 119, named_losses: [('ActivationMax Loss', -1173.5322),\n",
            " ('L-6.0 Norm Loss', 0.7033626),\n",
            " ('TV(2.0) Loss', 604.1972)], overall loss: -568.6316528320312\n",
            "Iteration: 120, named_losses: [('ActivationMax Loss', -1181.1222),\n",
            " ('L-6.0 Norm Loss', 0.70206773),\n",
            " ('TV(2.0) Loss', 609.471)], overall loss: -570.9491577148438\n",
            "Iteration: 121, named_losses: [('ActivationMax Loss', -1174.7911),\n",
            " ('L-6.0 Norm Loss', 0.7022595),\n",
            " ('TV(2.0) Loss', 605.8204)], overall loss: -568.2684936523438\n",
            "Iteration: 122, named_losses: [('ActivationMax Loss', -1182.836),\n",
            " ('L-6.0 Norm Loss', 0.7013088),\n",
            " ('TV(2.0) Loss', 610.7321)], overall loss: -571.4026489257812\n",
            "Iteration: 123, named_losses: [('ActivationMax Loss', -1174.3038),\n",
            " ('L-6.0 Norm Loss', 0.7047126),\n",
            " ('TV(2.0) Loss', 604.61304)], overall loss: -568.986083984375\n",
            "Iteration: 124, named_losses: [('ActivationMax Loss', -1183.8773),\n",
            " ('L-6.0 Norm Loss', 0.7038575),\n",
            " ('TV(2.0) Loss', 612.0924)], overall loss: -571.0810546875\n",
            "Iteration: 125, named_losses: [('ActivationMax Loss', -1175.5997),\n",
            " ('L-6.0 Norm Loss', 0.70342934),\n",
            " ('TV(2.0) Loss', 604.95044)], overall loss: -569.9459228515625\n",
            "Iteration: 126, named_losses: [('ActivationMax Loss', -1178.2792),\n",
            " ('L-6.0 Norm Loss', 0.7019254),\n",
            " ('TV(2.0) Loss', 607.4027)], overall loss: -570.174560546875\n",
            "Iteration: 127, named_losses: [('ActivationMax Loss', -1175.6598),\n",
            " ('L-6.0 Norm Loss', 0.70472896),\n",
            " ('TV(2.0) Loss', 606.6126)], overall loss: -568.3424682617188\n",
            "Iteration: 128, named_losses: [('ActivationMax Loss', -1183.6332),\n",
            " ('L-6.0 Norm Loss', 0.7043268),\n",
            " ('TV(2.0) Loss', 613.8771)], overall loss: -569.0517578125\n",
            "Iteration: 129, named_losses: [('ActivationMax Loss', -1176.9735),\n",
            " ('L-6.0 Norm Loss', 0.70401),\n",
            " ('TV(2.0) Loss', 608.28613)], overall loss: -567.9833984375\n",
            "Iteration: 130, named_losses: [('ActivationMax Loss', -1182.0792),\n",
            " ('L-6.0 Norm Loss', 0.7040807),\n",
            " ('TV(2.0) Loss', 612.6716)], overall loss: -568.7035522460938\n",
            "Iteration: 131, named_losses: [('ActivationMax Loss', -1179.8062),\n",
            " ('L-6.0 Norm Loss', 0.70482147),\n",
            " ('TV(2.0) Loss', 611.94763)], overall loss: -567.1536865234375\n",
            "Iteration: 132, named_losses: [('ActivationMax Loss', -1185.8842),\n",
            " ('L-6.0 Norm Loss', 0.7053225),\n",
            " ('TV(2.0) Loss', 615.60004)], overall loss: -569.5787963867188\n",
            "Iteration: 133, named_losses: [('ActivationMax Loss', -1181.215),\n",
            " ('L-6.0 Norm Loss', 0.70598656),\n",
            " ('TV(2.0) Loss', 610.63086)], overall loss: -569.878173828125\n",
            "Iteration: 134, named_losses: [('ActivationMax Loss', -1183.829),\n",
            " ('L-6.0 Norm Loss', 0.7061929),\n",
            " ('TV(2.0) Loss', 615.4754)], overall loss: -567.6473999023438\n",
            "Iteration: 135, named_losses: [('ActivationMax Loss', -1177.0592),\n",
            " ('L-6.0 Norm Loss', 0.7047238),\n",
            " ('TV(2.0) Loss', 608.22437)], overall loss: -568.130126953125\n",
            "Iteration: 136, named_losses: [('ActivationMax Loss', -1179.3727),\n",
            " ('L-6.0 Norm Loss', 0.70572984),\n",
            " ('TV(2.0) Loss', 610.749)], overall loss: -567.91796875\n",
            "Iteration: 137, named_losses: [('ActivationMax Loss', -1175.6344),\n",
            " ('L-6.0 Norm Loss', 0.7055002),\n",
            " ('TV(2.0) Loss', 605.2794)], overall loss: -569.6495361328125\n",
            "Iteration: 138, named_losses: [('ActivationMax Loss', -1181.0271),\n",
            " ('L-6.0 Norm Loss', 0.7057749),\n",
            " ('TV(2.0) Loss', 612.4342)], overall loss: -567.8870849609375\n",
            "Iteration: 139, named_losses: [('ActivationMax Loss', -1175.3766),\n",
            " ('L-6.0 Norm Loss', 0.7064197),\n",
            " ('TV(2.0) Loss', 605.696)], overall loss: -568.9741821289062\n",
            "Iteration: 140, named_losses: [('ActivationMax Loss', -1184.264),\n",
            " ('L-6.0 Norm Loss', 0.7073373),\n",
            " ('TV(2.0) Loss', 615.14453)], overall loss: -568.412109375\n",
            "Iteration: 141, named_losses: [('ActivationMax Loss', -1176.3141),\n",
            " ('L-6.0 Norm Loss', 0.7070747),\n",
            " ('TV(2.0) Loss', 604.759)], overall loss: -570.8480834960938\n",
            "Iteration: 142, named_losses: [('ActivationMax Loss', -1181.9701),\n",
            " ('L-6.0 Norm Loss', 0.70675635),\n",
            " ('TV(2.0) Loss', 608.66956)], overall loss: -572.59375\n",
            "Iteration: 143, named_losses: [('ActivationMax Loss', -1176.5231),\n",
            " ('L-6.0 Norm Loss', 0.70833814),\n",
            " ('TV(2.0) Loss', 607.5133)], overall loss: -568.3013916015625\n",
            "Iteration: 144, named_losses: [('ActivationMax Loss', -1185.879),\n",
            " ('L-6.0 Norm Loss', 0.7074724),\n",
            " ('TV(2.0) Loss', 615.84393)], overall loss: -569.3275756835938\n",
            "Iteration: 145, named_losses: [('ActivationMax Loss', -1179.0),\n",
            " ('L-6.0 Norm Loss', 0.7097336),\n",
            " ('TV(2.0) Loss', 609.3418)], overall loss: -568.948486328125\n",
            "Iteration: 146, named_losses: [('ActivationMax Loss', -1188.3105),\n",
            " ('L-6.0 Norm Loss', 0.70860666),\n",
            " ('TV(2.0) Loss', 618.17535)], overall loss: -569.4265747070312\n",
            "Iteration: 147, named_losses: [('ActivationMax Loss', -1178.3551),\n",
            " ('L-6.0 Norm Loss', 0.7068485),\n",
            " ('TV(2.0) Loss', 609.56036)], overall loss: -568.0878295898438\n",
            "Iteration: 148, named_losses: [('ActivationMax Loss', -1182.9047),\n",
            " ('L-6.0 Norm Loss', 0.70700663),\n",
            " ('TV(2.0) Loss', 612.39355)], overall loss: -569.8040771484375\n",
            "Iteration: 149, named_losses: [('ActivationMax Loss', -1176.2941),\n",
            " ('L-6.0 Norm Loss', 0.70871055),\n",
            " ('TV(2.0) Loss', 607.2701)], overall loss: -568.3152465820312\n",
            "Iteration: 150, named_losses: [('ActivationMax Loss', -1183.8927),\n",
            " ('L-6.0 Norm Loss', 0.70931935),\n",
            " ('TV(2.0) Loss', 614.27893)], overall loss: -568.9044189453125\n",
            "Iteration: 151, named_losses: [('ActivationMax Loss', -1177.0573),\n",
            " ('L-6.0 Norm Loss', 0.7095649),\n",
            " ('TV(2.0) Loss', 607.7772)], overall loss: -568.5704345703125\n",
            "Iteration: 152, named_losses: [('ActivationMax Loss', -1185.4222),\n",
            " ('L-6.0 Norm Loss', 0.7092651),\n",
            " ('TV(2.0) Loss', 614.59705)], overall loss: -570.115966796875\n",
            "Iteration: 153, named_losses: [('ActivationMax Loss', -1176.0585),\n",
            " ('L-6.0 Norm Loss', 0.70929295),\n",
            " ('TV(2.0) Loss', 606.308)], overall loss: -569.0411376953125\n",
            "Iteration: 154, named_losses: [('ActivationMax Loss', -1185.1224),\n",
            " ('L-6.0 Norm Loss', 0.7093779),\n",
            " ('TV(2.0) Loss', 613.4244)], overall loss: -570.9887084960938\n",
            "Iteration: 155, named_losses: [('ActivationMax Loss', -1177.7227),\n",
            " ('L-6.0 Norm Loss', 0.7098476),\n",
            " ('TV(2.0) Loss', 609.34705)], overall loss: -567.665771484375\n",
            "Iteration: 156, named_losses: [('ActivationMax Loss', -1186.3112),\n",
            " ('L-6.0 Norm Loss', 0.7103597),\n",
            " ('TV(2.0) Loss', 615.99005)], overall loss: -569.6107788085938\n",
            "Iteration: 157, named_losses: [('ActivationMax Loss', -1175.7816),\n",
            " ('L-6.0 Norm Loss', 0.70990586),\n",
            " ('TV(2.0) Loss', 606.9261)], overall loss: -568.1455688476562\n",
            "Iteration: 158, named_losses: [('ActivationMax Loss', -1187.3588),\n",
            " ('L-6.0 Norm Loss', 0.7107249),\n",
            " ('TV(2.0) Loss', 614.8231)], overall loss: -571.824951171875\n",
            "Iteration: 159, named_losses: [('ActivationMax Loss', -1178.044),\n",
            " ('L-6.0 Norm Loss', 0.7097101),\n",
            " ('TV(2.0) Loss', 609.08704)], overall loss: -568.2471923828125\n",
            "Iteration: 160, named_losses: [('ActivationMax Loss', -1183.5743),\n",
            " ('L-6.0 Norm Loss', 0.7101585),\n",
            " ('TV(2.0) Loss', 609.74963)], overall loss: -573.114501953125\n",
            "Iteration: 161, named_losses: [('ActivationMax Loss', -1177.601),\n",
            " ('L-6.0 Norm Loss', 0.710471),\n",
            " ('TV(2.0) Loss', 608.8946)], overall loss: -567.9959106445312\n",
            "Iteration: 162, named_losses: [('ActivationMax Loss', -1185.7709),\n",
            " ('L-6.0 Norm Loss', 0.709965),\n",
            " ('TV(2.0) Loss', 613.8322)], overall loss: -571.2286987304688\n",
            "Iteration: 163, named_losses: [('ActivationMax Loss', -1180.5692),\n",
            " ('L-6.0 Norm Loss', 0.7110831),\n",
            " ('TV(2.0) Loss', 610.32184)], overall loss: -569.5363159179688\n",
            "Iteration: 164, named_losses: [('ActivationMax Loss', -1188.7233),\n",
            " ('L-6.0 Norm Loss', 0.7123449),\n",
            " ('TV(2.0) Loss', 617.4955)], overall loss: -570.515380859375\n",
            "Iteration: 165, named_losses: [('ActivationMax Loss', -1175.7668),\n",
            " ('L-6.0 Norm Loss', 0.7102165),\n",
            " ('TV(2.0) Loss', 605.8772)], overall loss: -569.179443359375\n",
            "Iteration: 166, named_losses: [('ActivationMax Loss', -1185.1536),\n",
            " ('L-6.0 Norm Loss', 0.7108574),\n",
            " ('TV(2.0) Loss', 613.9166)], overall loss: -570.526123046875\n",
            "Iteration: 167, named_losses: [('ActivationMax Loss', -1176.1339),\n",
            " ('L-6.0 Norm Loss', 0.71158516),\n",
            " ('TV(2.0) Loss', 606.926)], overall loss: -568.496337890625\n",
            "Iteration: 168, named_losses: [('ActivationMax Loss', -1186.0864),\n",
            " ('L-6.0 Norm Loss', 0.71218514),\n",
            " ('TV(2.0) Loss', 615.5581)], overall loss: -569.816162109375\n",
            "Iteration: 169, named_losses: [('ActivationMax Loss', -1178.5382),\n",
            " ('L-6.0 Norm Loss', 0.71082073),\n",
            " ('TV(2.0) Loss', 609.15564)], overall loss: -568.6717529296875\n",
            "Iteration: 170, named_losses: [('ActivationMax Loss', -1185.2828),\n",
            " ('L-6.0 Norm Loss', 0.711733),\n",
            " ('TV(2.0) Loss', 614.53455)], overall loss: -570.0364990234375\n",
            "Iteration: 171, named_losses: [('ActivationMax Loss', -1182.7013),\n",
            " ('L-6.0 Norm Loss', 0.71129537),\n",
            " ('TV(2.0) Loss', 613.7565)], overall loss: -568.2335205078125\n",
            "Iteration: 172, named_losses: [('ActivationMax Loss', -1184.1278),\n",
            " ('L-6.0 Norm Loss', 0.71175337),\n",
            " ('TV(2.0) Loss', 613.16003)], overall loss: -570.2559814453125\n",
            "Iteration: 173, named_losses: [('ActivationMax Loss', -1181.647),\n",
            " ('L-6.0 Norm Loss', 0.71067363),\n",
            " ('TV(2.0) Loss', 610.9265)], overall loss: -570.009765625\n",
            "Iteration: 174, named_losses: [('ActivationMax Loss', -1184.4716),\n",
            " ('L-6.0 Norm Loss', 0.7114371),\n",
            " ('TV(2.0) Loss', 614.41235)], overall loss: -569.3477783203125\n",
            "Iteration: 175, named_losses: [('ActivationMax Loss', -1181.3387),\n",
            " ('L-6.0 Norm Loss', 0.71107763),\n",
            " ('TV(2.0) Loss', 610.6269)], overall loss: -570.0007934570312\n",
            "Iteration: 176, named_losses: [('ActivationMax Loss', -1184.4037),\n",
            " ('L-6.0 Norm Loss', 0.7123017),\n",
            " ('TV(2.0) Loss', 612.35065)], overall loss: -571.3407592773438\n",
            "Iteration: 177, named_losses: [('ActivationMax Loss', -1182.6776),\n",
            " ('L-6.0 Norm Loss', 0.711822),\n",
            " ('TV(2.0) Loss', 612.3626)], overall loss: -569.6032104492188\n",
            "Iteration: 178, named_losses: [('ActivationMax Loss', -1183.5197),\n",
            " ('L-6.0 Norm Loss', 0.71208376),\n",
            " ('TV(2.0) Loss', 611.8512)], overall loss: -570.9564208984375\n",
            "Iteration: 179, named_losses: [('ActivationMax Loss', -1181.9681),\n",
            " ('L-6.0 Norm Loss', 0.7121798),\n",
            " ('TV(2.0) Loss', 612.0893)], overall loss: -569.1666870117188\n",
            "Iteration: 180, named_losses: [('ActivationMax Loss', -1188.5886),\n",
            " ('L-6.0 Norm Loss', 0.7135595),\n",
            " ('TV(2.0) Loss', 616.6284)], overall loss: -571.2467041015625\n",
            "Iteration: 181, named_losses: [('ActivationMax Loss', -1178.0582),\n",
            " ('L-6.0 Norm Loss', 0.7098033),\n",
            " ('TV(2.0) Loss', 608.74097)], overall loss: -568.607421875\n",
            "Iteration: 182, named_losses: [('ActivationMax Loss', -1188.312),\n",
            " ('L-6.0 Norm Loss', 0.71315473),\n",
            " ('TV(2.0) Loss', 616.6254)], overall loss: -570.9734497070312\n",
            "Iteration: 183, named_losses: [('ActivationMax Loss', -1175.0623),\n",
            " ('L-6.0 Norm Loss', 0.710992),\n",
            " ('TV(2.0) Loss', 605.4863)], overall loss: -568.864990234375\n",
            "Iteration: 184, named_losses: [('ActivationMax Loss', -1184.2131),\n",
            " ('L-6.0 Norm Loss', 0.71321124),\n",
            " ('TV(2.0) Loss', 612.18225)], overall loss: -571.317626953125\n",
            "Iteration: 185, named_losses: [('ActivationMax Loss', -1179.0756),\n",
            " ('L-6.0 Norm Loss', 0.71370703),\n",
            " ('TV(2.0) Loss', 608.6764)], overall loss: -569.6854248046875\n",
            "Iteration: 186, named_losses: [('ActivationMax Loss', -1188.4158),\n",
            " ('L-6.0 Norm Loss', 0.71375316),\n",
            " ('TV(2.0) Loss', 615.62024)], overall loss: -572.081787109375\n",
            "Iteration: 187, named_losses: [('ActivationMax Loss', -1180.1697),\n",
            " ('L-6.0 Norm Loss', 0.71305764),\n",
            " ('TV(2.0) Loss', 604.93976)], overall loss: -574.5169067382812\n",
            "Iteration: 188, named_losses: [('ActivationMax Loss', -1187.0264),\n",
            " ('L-6.0 Norm Loss', 0.71480966),\n",
            " ('TV(2.0) Loss', 617.2662)], overall loss: -569.0453491210938\n",
            "Iteration: 189, named_losses: [('ActivationMax Loss', -1180.7657),\n",
            " ('L-6.0 Norm Loss', 0.71434915),\n",
            " ('TV(2.0) Loss', 608.7638)], overall loss: -571.28759765625\n",
            "Iteration: 190, named_losses: [('ActivationMax Loss', -1188.1053),\n",
            " ('L-6.0 Norm Loss', 0.7142964),\n",
            " ('TV(2.0) Loss', 617.10175)], overall loss: -570.2892456054688\n",
            "Iteration: 191, named_losses: [('ActivationMax Loss', -1178.5282),\n",
            " ('L-6.0 Norm Loss', 0.71426284),\n",
            " ('TV(2.0) Loss', 606.8707)], overall loss: -570.9432373046875\n",
            "Iteration: 192, named_losses: [('ActivationMax Loss', -1191.0548),\n",
            " ('L-6.0 Norm Loss', 0.7154893),\n",
            " ('TV(2.0) Loss', 621.0899)], overall loss: -569.2494506835938\n",
            "Iteration: 193, named_losses: [('ActivationMax Loss', -1180.0499),\n",
            " ('L-6.0 Norm Loss', 0.71288687),\n",
            " ('TV(2.0) Loss', 607.77466)], overall loss: -571.5623779296875\n",
            "Iteration: 194, named_losses: [('ActivationMax Loss', -1186.8944),\n",
            " ('L-6.0 Norm Loss', 0.71526605),\n",
            " ('TV(2.0) Loss', 617.05237)], overall loss: -569.1268310546875\n",
            "Iteration: 195, named_losses: [('ActivationMax Loss', -1180.0758),\n",
            " ('L-6.0 Norm Loss', 0.7151068),\n",
            " ('TV(2.0) Loss', 606.7831)], overall loss: -572.57763671875\n",
            "Iteration: 196, named_losses: [('ActivationMax Loss', -1185.5358),\n",
            " ('L-6.0 Norm Loss', 0.7147778),\n",
            " ('TV(2.0) Loss', 614.09076)], overall loss: -570.7302856445312\n",
            "Iteration: 197, named_losses: [('ActivationMax Loss', -1179.9702),\n",
            " ('L-6.0 Norm Loss', 0.7147411),\n",
            " ('TV(2.0) Loss', 606.9876)], overall loss: -572.2678833007812\n",
            "Iteration: 198, named_losses: [('ActivationMax Loss', -1189.3124),\n",
            " ('L-6.0 Norm Loss', 0.7155356),\n",
            " ('TV(2.0) Loss', 618.50214)], overall loss: -570.0946655273438\n",
            "Iteration: 199, named_losses: [('ActivationMax Loss', -1180.9794),\n",
            " ('L-6.0 Norm Loss', 0.7161755),\n",
            " ('TV(2.0) Loss', 607.1757)], overall loss: -573.0874633789062\n",
            "Iteration: 200, named_losses: [('ActivationMax Loss', -1182.154),\n",
            " ('L-6.0 Norm Loss', 0.7154855),\n",
            " ('TV(2.0) Loss', 611.9668)], overall loss: -569.4718017578125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe023a5d9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAFlCAYAAADYskK4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAESGxAwAkSExA0Bk/j8KEMJgpKbdeAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yFdhXjOKCNHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "MdxrNAvCCNHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "W3itOV2RCNHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Mw8Z5o2ACNHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}